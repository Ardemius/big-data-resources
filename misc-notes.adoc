= Misc notes about Data
Thomas SCHWENDER <icon:github[] https://github.com/Ardemius/[GitHub] / icon:twitter[role="aqua"] https://twitter.com/thomasschwender[@thomasschwender]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:resourcesdir: ./resources
:source-highlighter: highlightjs
:highlightjs-languages: asciidoc
// We must enable experimental attribute to display Keyboard, button, and menu macros
:experimental:
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 4
// To number the sections of the table of contents
//:sectnums:
// Add an anchor with hyperlink before the section title
:sectanchors:
// To turn off figure caption labels and numbers
:figure-caption!:
// Same for examples
//:example-caption!:
// To turn off ALL captions
// :caption:

toc::[]

Un bloc-notes pour persister toute ma veille techno sur la Data, et servant de base Ã  des articles plus structurÃ©s.

== Schema-on-Read vs Schema-on-Write

* 2018/09 : https://data-flair.training/forums/topic/what-is-the-difference-between-schema-on-read-and-schema-on-write/

    ** In traditional databases, the table's schema is imposed during the data load time, if the data being loaded does not conform to the schema then the data load is rejected, this process is know as Schema-on-Write. Here the data is being checked against the schema when written into the database(during data load). +
    Now in HIVE, the data schema is not verified during the load time, rather it is verified while processing the query. Hence this process in HIVE called Schema-on-Read. +
    Now, which way is better? Schema-on-Read or Schema-on-Write?

        ** *Schema-on-Read*: +
        Schema-on-Read helps in very fast initial data load, since the data does not have to follow any internal schema(internal database format) to read or parse or serialize, as it is just a copy/move of a file.
        This type of movement of data is more flexible incase of huge data or having two schemas for same underlying data.

        ** *Schema-on-Write*: +
        Schema-on-Write helps in faster performance of the query, as the data is already loaded in a particular format and it is easy to locate the column index or compress the data. However, it takes longer time to load data into the database.

    ** So, in scenarios of large data load or where the schema is not known at load time and there are no indexes to apply, as the query is not formulated, Schema-on-Read(property of HIVE) is more efficient than Schema-on-write.

== Normalisation et dÃ©normalisation des donnÃ©es

* https://stph.scenari-community.org/bdd/0/co/optUC004.html

    ** *Normalisation des donnÃ©es* : processus qui permet d'optimiser un modÃ¨le logique afin de le *rendre non redondant*. Ce processus conduit Ã  la fragmentation des donnÃ©es dans plusieurs tables.
        *** bon site sur la normalisation des donnÃ©es et les *diffÃ©rentes formes normales* : https://www.ionos.fr/digitalguide/hebergement/aspects-techniques/normalisation-base-de-donnees/

    ** *DÃ©normalisation des donnÃ©es* : Processus consistant Ã  regrouper plusieurs tables liÃ©es par des rÃ©fÃ©rences, en une seule table, en rÃ©alisant statiquement les opÃ©rations de jointure adÃ©quates. +
    L'objectif de la dÃ©normalisation est d'amÃ©liorer les performances de la BD en recherche sur les tables considÃ©rÃ©es, en implÃ©mentant les jointures plutÃ´t qu'en les calculant.
        *** *Quand utiliser la dÃ©normalisation* : Un schÃ©ma doit Ãªtre dÃ©normalisÃ© lorsque les performances de certaines recherches sont insuffisantes et que cette insuffisance Ã  pour cause des jointures.

== 2022/10/21 - Demo de Couchbase Capella

J'ai suivi la dÃ©mo de Couchbase Capella via leur offre d'essai (trial de 30 jours) de la solution.

VidÃ©o explicative : https://www.youtube.com/watch?v=46715VbaHvk

.Comparaison des concepts entre une BDDR et Couchbase
[cols="1,1", options="header"] 
|===
|Relationel model 			|Couchbase
|Server	                    |Cluster
|Database	                |Bucket
|Schema		                |Scope
|Table		                |Collection
|Row		                |Document (JSON or BLOB)
|===

.Exemple
image:20221021_couchbase-capella-demo_01.jpg[]

Why the creation of the index is not done automatically ?

    * Because *manipulating the document using only the ID* is *faster* because using internally the *key / value engine*, which *does NOT require any indexes*.
        ** This works pretty well when you can get the ID of the document

[source,java]
----
// guessing the UserHistory ID using the user's id ('123-hist')
UserHistory hist = userHistoryCollection.get(user.getId() + "-hist")
----

== 2022/05/12 - Le Comptoir OCTO x Dataiku x Snowflake - Comment crÃ©er plus de valeur et dÃ©velopper la collaboration a partir de donnÃ©es enrichies ?

https://fr.slideshare.net/OCTOTechnology/le-comptoir-octo-x-dataiku-x-snowflake-comment-crer-plus-de-valeur-et-developper-la-collaboration-a-partir-de-donnes-enrichies/OCTOTechnology/le-comptoir-octo-x-dataiku-x-snowflake-comment-crer-plus-de-valeur-et-developper-la-collaboration-a-partir-de-donnes-enrichies

* PrÃ©sentation d'une architecture de solution basÃ©e sur Snowflake et DataÃ¯ku, avec le soutien d'OCTO Technology

== 2023/01/09 - Rapport tendances 2023 par Didier Girard, section Data

* https://www.linkedin.com/pulse/rapport-tendances-2023-didier-girard : section Data de l'article

* Un des enjeux majeurs de 2023 est de maÃ®triser le *management* et la *gouvernance de la data*

* RÃ´les de la Data : 

    ** *Gouvernance et usage de la donnÃ©e* : 

        *** *CDO* : +
        Il doit construire une gouvernance de la donnÃ©e alignÃ©e avec la stratÃ©gie business de l'entreprise. Ce doit Ãªtre un manager (ce n'est pas un techos), quelqu'un qui a une vision sur la faÃ§on de mettre la donnÃ©e au service du business, qui embarquera les Ã©quipes, les acculturera, qu'il s'agisse des employÃ©s ou de ses pairs de la direction.

        *** *Data Domain Owner* : +
        Chaque grand domaine de l'entreprise est responsable de ses donnÃ©es. Cela devient mÃªme flagrant dÃ¨s lors qu'on commence Ã  raisonner en termes de *Data Mesh*. +
        Les patrons des finances, du marketing, etc. sont donc de facto des Data Domain Owners, un rÃ´le qu'ils dÃ©lÃ¨guent Ã  des personnes au sein de leurs Ã©quipes. +
        Ce sont des personnes qui ont une connaissance approfondie de la donnÃ©e mÃ©tier, comprennent les enjeux du data-driven, ou comment et pourquoi il faut partager la donnÃ©e pour la valoriser. +
        Ces personnes doivent dÃ©crire les jeux de donnÃ©es au sein des data catalogs, ou encore dÃ©terminer qui peut y accÃ©der et sous quelles conditions (dans le cadre d'un framework de partage dÃ©terminÃ© par le CDO).

        *** *Data Stewards* : +
        Les Data Stewards jouent un rÃ´le protÃ©iforme, puisqu'ils aident les autres acteurs Ã  dÃ©finir les normes et processus de collecte, Ã  s'assurer de la qualitÃ© des donnÃ©es, Ã  rÃ©soudre certains problÃ¨mesâ€¦ +
        Ce sont eux aussi qui vont assister les utilisateurs de donnÃ©es pour s'assurer que ces derniÃ¨res sont bien utilisÃ©es de maniÃ¨re appropriÃ©e, conformÃ©ment aux rÃ¨gles de l'entreprise.

    ** *Fabrication et l'exploitation des produits et plateformes Data* : 

        *** *Data architects* : +
        Ils dessinent les grandes lignes de la plateforme, ses principes directeurs et dÃ©finissent l'articulation entre les composants. Ils possÃ¨dent des connaissances globales sur l'Ã©cosystÃ¨me technique, sont conscients des spÃ©cificitÃ©s techniques et donc des avantages et inconvÃ©nients des principaux produits, langages et types d'architecture et peuvent aider Ã  coder si besoin.

        *** *Data engineers* : +
        Ils dÃ©finissent, dÃ©veloppent, mettent en place et maintiennent les outils et infrastructures permettant l'analyse de la donnÃ©e. SpÃ©cialisÃ©s dans les problÃ©matiques de croisement et de gestion des donnÃ©es Ã  large Ã©chelle, ce sont eux qui vont implÃ©menter les idÃ©es des Data Analysts.

        *** *Data scientists* : +
        Les Data Scientists construisent des modÃ¨les mathÃ©matiques de machine learning pour rÃ©pondre Ã  des problÃ©matiques mÃ©tier. Dans la majoritÃ© des cas, ils s'appuieront sur des modÃ¨les existants qu'ils personnaliseront pour rÃ©pondre Ã  des enjeux opÃ©rationnels. +
        Mais surtout, le rÃ´le des Data Scientists ne s'arrÃªte plus Ã  la mise au point des modÃ¨les ; dÃ©sormais, ils travaillent conjointement avec les ML Engineers pour s'assurer que leur modÃ¨le produise des rÃ©sultats cohÃ©rents et pertinents tout au long de leur cycle de vie.

        *** *ML engineer* : +
        Ils appliquent les principes du DataOps Ã  la data science : industrialisation, fiabilitÃ©, observabilitÃ©, etc. Ils mettent en place toute l'infrastructure pour que les Data Scientists puissent tester et publier leur modÃ¨le de faÃ§on automatisÃ©e, mais aussi obtenir le feedback nÃ©cessaire pour mettre en Å“uvre de l'amÃ©lioration continue. Ce sont eux qui vont mettre les solutions IA Ã  l'Ã©chelle et optimiser la performance globale des modÃ¨les. De plus en plus, l'aspect IA responsable devrait entrer dans leur champ de prÃ©occupations.

        *** *Data Analysts* (et Ã  terme *TOUS les utilisateurs*) : +
        Les Data Analysts manipulent la donnÃ©e pour en tirer des enseignements clÃ©s, afin de rÃ©soudre des problÃ¨mes ou de prendre des dÃ©cisions mieux informÃ©es. S'il s'agit aujourd'hui de rÃ´les distincts, il est probable qu'on assiste dans le futur, avec l'acculturation de l'ensemble des collaborateurs Ã  la donnÃ©e et la mise Ã  disposition d'outils self-service "intelligents" (avec de l'IA pour des requÃªtes en langage naturel et des analyses poussÃ©es), Ã  une disparition de ce terme. On Ã©voquera alors plutÃ´t des centaines de millions de personnes analysant de la donnÃ©e dans le cadre de leur travail quotidien, des graphistes, de propriÃ©taires de pizzÃ©rias, de chefs de produits...

* *Data mesh* : 
    ** Data mesh : une architecture particuliÃ¨rement bien adaptÃ©e aux systÃ¨mes basÃ©s sur les produits
    ** La notion de "mesh", le maillage, *favorise la crÃ©ation de produits rÃ©pondant Ã  des besoins spÃ©cifiques*. PlutÃ´t que de vouloir centraliser l'ensemble des donnÃ©es, l'approche data mesh laisse les responsables de domaines (Domain Data Owners) gÃ©rer leurs donnÃ©es, leur qualitÃ©, qui peut y accÃ©der et sous quelles conditionsâ€¦ +
    Les responsables produits vont crÃ©er des produits sur la base de ces donnÃ©es, et pourront Ãªtre clients des donnÃ©es d'autres domaines. Chaque produit peut Ã©voluer indÃ©pendamment en fonction des Ã©volutions des besoins clients et de l'enrichissement de chaque domaine.
    ** Ce dÃ©couplage *favorise aussi Ã  son tour les architectures "event-driven"*, les domaines informant le reste du SI d'Ã©vÃ©nements se produisant en leur sein.
    ** Cette *approche fÃ©dÃ©rÃ©e plutÃ´t que centralisÃ©e* donne ainsi plus de latitude - qui ne doit pas Ãªtre confondue avec de l'anarchie, oÃ¹ chacun ferait ce qu'il souhaite dans son coin. C'est pourquoi il est primordial d'instaurer des rÃ¨gles de gouvernance, de mettre en place les rÃ´les et responsabilitÃ©s nÃ©cessaires, mais aussi une plateforme et un outillage communs qui vont faciliter la crÃ©ation et la maintenance de ces produits data.

* *Data management* : une discipline Ã©troitement liÃ©e Ã  lâ€™informatique, qui consiste Ã  mettre en place lâ€™outillage nÃ©cessaire pour gÃ©rer, sÃ©curiser et partager les donnÃ©es.

* *Data governance* : concerne les hommes et lâ€™usage de la donnÃ©e : quels sont les rÃ´les et responsabilitÃ©s, quelles sont les rÃ¨gles dâ€™accÃ¨s Ã  la donnÃ©e, les contraintes lÃ©gales et Ã©thiques respecter, pour quels usagesâ€¦
    ** Un de ses principaux dÃ©fis : trouver le bon Ã©quilibre entre l'accÃ¨s et le contrÃ´le des donnÃ©es.
    ** outils associÃ©s : catalogues et dictionnaires de donnÃ©es, outils de lignage et d'audit des donnÃ©es, outils de qualitÃ© et de sÃ©curitÃ© des donnÃ©es.

* *Le partage de la Data* : 
    ** La valorisation de la donnÃ©e ne sera possible que si les Data Domain Owners jouent le jeu du partage. +
    Contrairement Ã  lâ€™or noir, *la donnÃ©e ne sâ€™Ã©puise pas quand on la consomme*, elle crÃ©e de nouvelles donnÃ©es et enrichit Ã  la fois son producteur et son consommateur.
    ** *Partager la donnÃ©e* est la condition sine qua non dâ€™une *stratÃ©gie data-driven*.

* *DataOps et MLOps remplacent progressivement Datalabs et Data Factories*

    ** La donnÃ©e en tant que terrain de *jeu* et *dâ€™expÃ©rimentation* touche Ã  sa *fin*. +
    La crise Ã©conomique aidant, il sâ€™agit aujourdâ€™hui dâ€™*industrialiser les projets*, de les dÃ©ployer Ã  lâ€™Ã©chelle et de dÃ©montrer la capacitÃ© Ã  soutenir des processus business et crÃ©er de la valeur.
    ** *DataOps* et *MLOps* fournissent le guide dâ€™utilisation pour mettre en place du CI/CD, de lâ€™automatisation et de l'observabilitÃ©, toutes conditions nÃ©cessaires Ã  une *approche industrielle*.

* *FinOps et Data*
    ** Les projets data ne doivent plus dÃ©marrer sans une composante FinOps, de faÃ§on Ã  pouvoir attribuer les coÃ»ts aux diffÃ©rents domaines mÃ©tiers.
    ** La dÃ©marche FinOps sâ€™assurera aussi que les bonnes pratiques sont respectÃ©es tout au long du projet, par exemple la *mise en place de seuils et de quotas* qui dÃ©clencheront des alertes, voire stopperont un service.

* *SQL est le langage universel de la Data*
    ** Tous les systÃ¨mes qui stockent ou exposent de la donnÃ©e offrent dÃ©sormais une prise en charge de SQL
        *** ce qui permet aux utilisateurs d'Ã©crire des requÃªtes qui combinent des donnÃ©es provenant de plusieurs sources et d'effectuer des analyses avancÃ©es. 
    ** Les avancÃ©es rÃ©centes vont jusqu'Ã  l'*intÃ©gration de modÃ¨les IA et de ML directement dans le langage*.

* *Lâ€™ELT dÃ©trÃ´ne lâ€™ETL*
    ** Lâ€™avÃ¨nement des nouvelles architectures de donnÃ©es privilÃ©gie le plus souvent le *chargement des donnÃ©es brutes au sein dâ€™un datalake*. 
    ** Lâ€™Ã©tape de transformation est rÃ©alisÃ©e ensuite, si elle sâ€™avÃ¨re nÃ©cessaire, pour injecter les donnÃ©es au sein du datawarehouse. +
    De cette faÃ§on, les *data scientists auront accÃ¨s aux donnÃ©es brutes* et, si de nouveaux besoins analytiques Ã©mergent, de nouvelles transformations pourront Ãªtre opÃ©rÃ©es Ã  partir des donnÃ©es brutes.
    ** D'oÃ¹ un bouleversement du marchÃ© des outils dâ€™ingestion de donnÃ©es et l'apparition d'*outils se consacrant spÃ©cifiquement Ã  la transformation*, dont le plus populaire est le *framework dbt*

        *** *dbt* : permet de dÃ©crire les transformations de donnÃ©es de faÃ§on modulaire, de les tester et de les documenter ; la documentation produite intÃ©grant automatiquement le lignage de la donnÃ©e.
        *** La qualitÃ© du code pouvant laisser Ã  dÃ©sirer, le framework *Dataform* (rachetÃ© puis intÃ©grÃ© Ã  Google Cloud Platform) a Ã©tÃ© crÃ©Ã© avec pour objectif d'y remÃ©dier, MAIS est encore trÃ¨s jeune et doit progresser

IMPORTANT: DANS TOUS LES CAS, *le dÃ©couplage EL & T paraÃ®t maintenant actÃ©*.

* *Data Contracts*
    ** Autre concept poussÃ© par lâ€™essor du data mesh et des architectures distribuÃ©es

    ** Les Data Contracts sont des *accords entre les producteurs de donnÃ©es et les consommateurs de donnÃ©es* qui dÃ©crivent les attentes et les exigences en matiÃ¨re de qualitÃ© et de cohÃ©rence des donnÃ©es.
        *** Les contrats sont conÃ§us pour rÃ©soudre le problÃ¨me des changements de schÃ©ma inattendus, qui peuvent causer des problÃ¨mes de qualitÃ© des donnÃ©es et perturber les systÃ¨mes aval.

* *Les bases orientÃ©es documents alliÃ©es du "move to cloud"*
    ** *pas de schÃ©ma fixe* pour organiser les donnÃ©es, au lieu de cela stockage dans des documents, Ã  savoir des collections pouvant avoir diffÃ©rentes structures et Ãªtre facilement modifiÃ©es.
    ** gÃ¨rent un large Ã©ventail de types de donnÃ©es, notamment des donnÃ©es structurÃ©es, semi-structurÃ©es et non structurÃ©es.
    ** trÃ¨s *performantes* : capables de traiter de grands volumes de donnÃ©es et des niveaux Ã©levÃ©s de dÃ©bit
    ** *Hautement disponibles* et peuvent Ãªtre facilement dÃ©ployÃ©es sur une infrastructure basÃ©e sur le cloud

    ** MAIS, PAS adaptÃ©es Ã  tous les usages, et nÃ©cessitent un Ã©tat d'esprit et des compÃ©tences spÃ©cifiques diffÃ©rentes de celles associÃ©es aux dÃ©veloppements "traditionnels"S

* *"No Backend" et services managÃ©s*
    ** il sâ€™agit de se concentrer sur le fonctionnel, et de laisser le management de la base Ã  un service cloud, qui rÃ©alisera la maintenance, la sauvegarde, les montÃ©es de version, etc.
    ** Le moteur PostgreSQL est ainsi proposÃ© par de multiples services, chez les fournisseurs de cloud, mais aussi dans lâ€™open source, avec Supabase, une solution crÃ©Ã©e comme une alternative Ã  Firebase (Google) et qui monte dans lâ€™Ã©cosystÃ¨me.
        *** Il s'agit de 2 solutions dites "Backend as a ServiceS"

* *Data Lakehouse, lâ€™autre nom dâ€™une Data Platform*
    ** Exemples : Databricks, Starburst, Cloudera, Snowflake

* *De la data analytique Ã  la data opÃ©rationnelle*
    ** La capacitÃ© Ã  crÃ©er des produits avec de la data raffinÃ©e commence Ã  sortir du cadre analytique pour revenir dans le cadre opÃ©rationnel. 
    ** Un cas dâ€™usage de plus en plus frÃ©quent concerne les *rÃ©fÃ©rentiels clients uniques*, constituÃ©s au sein dâ€™une data platform Ã  partir de plusieurs bases clients de diffÃ©rents systÃ¨mes opÃ©rationnels (CRM, ventes, abonnements, SAV, etc.). +
    Les donnÃ©es rÃ©conciliÃ©es, nettoyÃ©es, dÃ©doublonnÃ©es, peuvent Ãªtre rÃ©injectÃ©es pour venir servir des systÃ¨mes opÃ©rationnels, sous forme de *produits data* mis Ã  disposition au sein dâ€™un *hub de donnÃ©es*, ou injectÃ©es directement dans une application (opÃ©ration de type *reverse-ETL*).

== 2023/03/30 : Les 3 grands facteurs clÃ©s de succÃ¨s d'une entreprise data driven

* https://www.wenvision.com/les-facteurs-cles-de-succes-dune-entreprise-data-driven/

* L'organisation data par domaine permet de dÃ©sengorger la gestion des donnÃ©es d'une Ã©quipe centralisÃ©e et valoriser la connaissance. Elle dÃ©place la responsabilitÃ© auprÃ¨s des domaines ce qui offre en plus d'une expertise technique une expertise mÃ©tier. La crÃ©ation d'Ã©quipes pluridisciplinaires doit favoriser cette innovation. On parle souvent de *Data Mesh*, pour Ã©voquer cette dÃ©centralisation des donnÃ©es.

== 2023/04/26 : ma rÃ©action Ã  l'article de Didier Girard "L'IA gÃ©nÃ©rative sera au data catalogue ce que Google a Ã©tÃ© Ã  Yahoo"

L'article de Didier est disponible sur le blog de WEnvision : https://www.wenvision.com/lia-generative-sera-au-data-catalogue-ce-que-google-a-ete-a-yahoo/

Un article trÃ¨s intÃ©ressant de Didier, dont je partage pleinement les conclusions, avec beaucoup de curiositÃ© sur l'Ã©volution de ce domaine Ã  (trÃ¨s) court terme ðŸ˜‰ 

A l'heure actuelle, la "vraie" "big" data a lieu quand les metadata elles-mÃªmes doivent Ãªtre traitÃ©es comme de la "big data". +
Depuis quelques temps, nous sommes passÃ©s d'une gestion "passive" des metadata (les plateformes de metadata / data catalog Ã©taient dans l'attente d'une action humaine pour la saisie de metadata et / ou leur catÃ©gorisation) Ã  des "active metadata platforms" comme les appelle le Gartner. +
Ces derniÃ¨res collectent en continu toutes les metadata qu'elles peuvent trouver sur le SI, d'oÃ¹ une explosion de la volumÃ©trie associÃ©e.

RÃ©sultat : il devient trÃ¨s difficile (voire impossible) de cataloguer cette derniÃ¨re en amont de la crÃ©ation / ingestion des metadata. +
Il nous faut donc un moyen de le faire soit au moment de la crÃ©ation de la metadonnÃ©e, soit plus tard, Ã  la demande, au moment ou on a besoin de se servir des metadata. +
Dans le 1er cas, le problÃ¨me est de trouver sur quelle base il est possible d'identifier / catÃ©goriser cette metadata ? +
Fasse Ã  des volumes de metadata trÃ¨s consÃ©quents et trÃ¨s variables, une catÃ©gorisation "statique" prÃ©dÃ©finie en amont n'est plus possible ou adÃ©quate, il faut donc se baser sur un ensemble de rÃ¨gles dont le but est d'aboutir par calcul Ã  une catÃ©gorisation. +
Souci : ce "calcul de catÃ©gorisation" est seulement valable Ã  un instant "t", car forcÃ©ment dÃ©pendant du volume de meta-donnÃ©e. +
Avec l'avÃ¨nement des "active metadata", la catÃ©gorisation dÃ©terminÃ©e Ã  un instant "t" ne sera probablement plus correct Ã  un instant "t + x" synonyme d'un pourcentage (consÃ©quent) de metadata supplÃ©mentaires. +
DÃ¨s lors, c'est la 2e solution qui paraÃ®t la plus pertinente : une catÃ©gorisation Ã  la demande.

Et lÃ  je rejoins complÃ¨tement l'avis de Didier, le catalogage "statique" n'est plus possible et doit Ãªtre remplacÃ© par un moyen efficace d'aboutir Ã  cette catÃ©gorisation Ã  la demande : un algorithme rappelant le fonctionnement d'un moteur de recherche. +
C'est Ã  ce moment qu'on voit l'IA gÃ©nÃ©rative entrer en scÃ¨ne.

Les grandes Ã©tapes d'Ã©volution des data catalog ont Ã©tÃ© : 

    * Data Catalog 1.0: la gestion des metadata (identification, catÃ©gorisation, etc.) est directement l'affaire des Ã©quipes techniques
    * Data Catalog 2.0: on passe Ã  une gestion pilotÃ©e par des Ã©quipes dÃ©diÃ©es (nos data stewards) en lien Ã©troit avec le mÃ©tier
    * Data Catalog 3.0: Devant le nombre toujours croissant de metadata, on donne les moyens Ã  une communautÃ© Ã©tendue d'utilisateurs d'analyser les metadata.

Aujourd'hui, nous arrivons Ã  l'aube du Data Catalog "4.0" : les metadata deviennent tout simplement trop nombreuses pour un traitement "humain" ou crÃ©Ã© par des humains (les rÃ¨gles changeraient trop vites), nous avons besoin d'une aide, d'une "prÃ©-catÃ©gorisation" effectuÃ©e par la machine, c'est lÃ  que l'IA gÃ©nÃ©rative intervient : nous crÃ©er / suggÃ©rer les catÃ©gories les plus pertinentes (entre autres), mais Ã  la demande. +
Mais est-ce encore un data "catalog" ? Comme le dit Didier, on se trouve davantage face Ã  un "metadata search engine".

DÃ¨s lors, la question que je me pose est : comment valider cette catÃ©gorisation effectuÃ©e Ã  la demande, sachant qu'elle est susceptible de changer trÃ¨s rapidement, avec la prochaine ingestion d'un +x0% de metadata d'un coup (ou plus encore) qui viendra modifier toutes les catÃ©gories prÃ©cÃ©demment calculÃ©es par l'algo ? +
Une interventation de validation serait impossible ou trÃ¨s compliquÃ©e car trÃ¨s (trop) limitÃ©e dans le temps : valider une catÃ©gorisation stable sur 1 mois soit, 1 semaine pourquoi pas, mais si cela doit passer Ã  plusieurs fois par jour ? +
DÃ¨s lors, accepterait-on de croire la catÃ©gorisation rÃ©alisÃ©e par la machine "sur parole", sans contrÃ´le humain ? +
Contrairement Ã  une "recherche Google classique", qui est avant tout "indicative", les metadata sont Ã  la base de process opÃ©rationnels et mÃ©tier : une information "indicative" n'est pas suffisante, il faut une information "validÃ©e". +
Comment valider cette information, son "sens mÃ©tier" ? +
Pourrait-on imaginer des "Tests Unitaires de catÃ©gorisation de donnÃ©es" ? Mais, ne connaissant ni le rÃ©sultat Ã  l'avance (la catÃ©gorie !) ni la mÃ©canique de rÃ©solution de l'algo, l'Ã©criture de ces derniers me semble difficile.

J'ai hÃ¢te de voir comment va Ã©voluer ce milieu dans les mois Ã  venir, et Ã  quoi vont ressembler les prochains data catalog.

== 2023/02/07 - Jordan TIGANI (MotherDuck, l'Ã©diteur de DuckDB) : Big Data is dead

URL de l'article : https://motherduck.com/blog/big-data-is-dead/

* Jordan utilise / cite le comparateur bien connu "DB Engines" pour comparer les perfs de certaines BDDs.

* Customer data sizes followed a power-law distribution. The largest customer had double the storage of the next largest customer, the next largest customer had half of that, etc. So while there were customers with hundreds of petabytes of data, the sizes trailed off very quickly. There were *many thousands of customers* who paid *less than $10 a month for storage*, which is *half a terabyte*. Among customers who were using the service heavily, the *median data storage size* was much less than *100 GB*.

* He (GCP investissor ?) found that the *largest B2B companies* in his portfolio had around *a terabyte of data*, while the *largest B2C companies* had around *10 Terabytes of data*. +
-> Most, however, had *far less data*.

* *Modern cloud data platforms all separate storage and compute*, which means that customers are not tied to a single form factor. This, more than scale out, is likely the single *most important change in data architectures* in the last 20 years.
    ** *Instead of "shared nothing" architectures* which are hard to manage in real world conditions, *shared disk architectures* let you grow your storage and your compute independently. +
    The rise of scalable and reasonably fast object storage like S3 and GCS meant that you could relax a lot of the constraints on how you built a database.

* *The amount of data processed for analytics workloads is almost certainly smaller than you think*. Dashboards, for example, very often are built from aggregated data. People look at the last hour, or the last day, or the last week's worth of data. Smaller tables tend to be queried more frequently, giant tables more selectively.

* A couple of years ago I did an analysis of BigQuery queries, looking at customers spending more than $1000 / year. *90% of queries processed less than 100 MB of data*.

* A huge percentage of the data that gets processed is less than 24 hours old. By the time data gets to be a week old, it is probably 20 times less likely to be queried than from the most recent day.

* One definition of *"Big Data" is "whatever doesn't fit on a single machine*.. By that definition, the number of workloads that qualify has been decreasing every year.

* An alternate definition of *Big Data is "when the cost of keeping data around is less than the cost of figuring out what to throw away."* 
    ** I like this definition because it encapsulates why people end up with Big Data. It isn't because they need it; they just haven't bothered to delete it. +
    If you think about many data lakes that organizations collect, they fit this bill entirely: giant, messy swamps where no one really knows what they hold or whether it is safe to clean them up.

* Some questions that you can ask to *figure out if you're a "Big Data One-Percenter"*:
    ** Are you really generating a huge amount of data?
    ** If so, do you really need to use a huge amount of data at once?
    ** If so, is the data really too big to fit on one machine?
    ** If so, are you sure you're not just a data hoarder?
    ** If so, are you sure you wouldn't be better off summarizing?

== 2023/01/24 - Ryan BOYD (MotherDuck, l'Ã©diteur de DuckDB) : How to analyse SQLite databases in DuckDB

* https://motherduck.com/blog/analyze-sqlite-databases-duckdb/

* *DuckDB* is often referred to as the *'SQLite for analytics.'* +
This analogy helps us understand several key properties of DuckDB: 
    ** it's for analytics (OLAP), 
    ** it's embeddable, 
    ** it's lightweight, 
    ** it's self-contained 
    ** and it's widely deployed. +
-> Okay, the latter may not be a given yet for DuckDB, but SQLite says it's likely the most widely used and deployed database engine and, with the rising popularity of analytics, it's quite possible DuckDB will eventually be competitive.

* There are some noticeable differences between SQLite and DuckDB in how data is stored. 
    ** *SQLite*, as a data store *focused on transactions*, *stores data row-by-row* while *DuckDB*, as a *database engine for analytics*, stores *data by columns*. 
    ** Additionally, SQLite doesn't strictly enforce types in the data -- this is known as being weakly typed (or flexibly typed).

== 2023/05/11 - Tech Rocks - Modern Data Stack

AnimÃ© par : Marie GRAPPE (Choose - Head of Data), Julieu GOULLEY (Fivetran - Senior Solution Architet), Thomas LAPORTE (devoteam - CTO France)

* MDS : Modern Data Stack

image:20230511_tech-rocks_modern-data-stack_01.jpg[]

* Le MDS est une solution Cloud, avec peu de configuration technique et qui ouvre donc la barriÃ¨re d'entrÃ©e pour plus d'utilisateurs.
* Les caractÃ©ristiques clÃ©s de la MDS : 
    ** Cloud-First
    ** ETL remplacÃ© par une approche ELT
    ** SQL-based
    ** EntiÃ¨rement managÃ©
        *** l'automatisation de l'accÃ¨s aux donnÃ©es est un des piliers de la MDS. Vous n'avez plus Ã  crÃ©er et manager des pipelines vous-mÃªmes.

* Thomas LAPORTE : PlutÃ´t qu'une "stack", la MDS est davantage une collection d'outils

* Julien Goulley : "DBT qui est un outil de transformation qui permet d'Ã©crire des modÃ¨les en SQL [...]"

Et maintenant un autre article, 2022/07, *critique de la MDS* : https://anaselk.com/p/modern-data-stack-dead/

    * Il en ressort ce schÃ©ma, oÃ¹ une approche plus raisonnable que la MDS est proposÃ©e (appelÃ©e "Postmodern Data Stack" par l'auteur, Lauren Balik) : +
    image:202207_modern-data-stack-vs-more-reasonable-stack.jpg[]

Pour un autre *recensement des technologies derriÃ¨re la Modern Data Stack*, voir ce site : https://notion.castordoc.com/modern-data-stack-guide +
image:20230511_tech-rocks_modern-data-stack_02.jpg[]

== 2023/05/12 - Starburst Academy : The difference between data lakes and data lakehouses

* URL : https://www.youtube.com/watch?v=k1cch-6bZhM

* *Modern data formats* replaced traditional old Hive format. +
Those new modern data formats : 
    ** Apache *Iceberg*
    ** Databricks *Delta Lake*
    ** Apache *Hudi*

* Hive tables lack ACID compliance and version control -> not the case of those modern data formats

* *Those new data formats are what make a lakehouse a lakehouse*.
    ** With Hive, we create a data lake
    ** with those formats, we create a data lakehouse

* Compared to data lake, those new formats handle better performance, data modification and schema evolution
    ** Ces nouveaux formats permettent des performances proches des data warehouse ou des BDDs, tout en utilisant un stockage objet, comme les data lakes.

* Data lakehouse improves the *reporting structure*.
    ** data lakes store metadata limited to : location, format, structure BUT they do NOT record a comprehensive end to end record of all changes made to a table.
    
    ** On the other side, data *lakehouses* store *large amounts of metadata* painting a *comprehensive picture of the system*, including record by record details of : 
        *** every modifications
        *** every updates
        *** every deletions
    ** Those metadata are stored in a set of hierarchically structured files : *manifest files*
        *** Manifest files capture changes in the state of the dataset, providing the ability to record an accurate up-to-date account of the changes occurring in the table at any given time : inserts, deletions, updates, schema migrations, partition updates +
        image:20230512_starburst-academy_data-lakehouse-modern-data-formats_01.jpg[]

* How Iceberg uses metadata manifest files to create a transactional layer on top of traditional data lake storage : 
    ** if a change is made TO THE DATA (let's a file persistance in this example) -> a manifest file is created that references a specific section of the data
    ** multiple manifest files are referenced in a manifest list
    ** manifest list is contained in a metadata file
    ** this metadata file is held in the Iceberd catalog

    ** The metadata held in the lakehouse ~ a database transaction log that sits on top of the traditional cloud object storage (this last making up a data lake)

* Collectively *those manifest files create a kind of snapshot*
    ** Iceberg calls them just that : *Snapshot files*
    ** Delta lake uses the Delta log in a similar way

    ** These snapshots detailed the points at which the changes are made
        *** So they can be used to query the database at a particular point in time, facilitate schema and partition evolution, or roll back changes

* Those numerous metadata and their possibilities are what make the differences between data lake and data lakehouse : 
    ** record level updates
    ** ACID compliance
    ** transaction support
    ** data concurrency support

image:20230512_starburst-academy_data-lakehouse-modern-data-formats_02.jpg[]





